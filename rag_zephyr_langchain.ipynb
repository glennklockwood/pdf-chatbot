{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "question = \"Where are paint bubbles appearing?\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8EKMit4WNDY8"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "loader = PyMuPDFLoader(\"input_files/sample.pdf\")\n",
        "docs = loader.load()\n",
        "docs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmsXOf59Pmm-"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=200, chunk_overlap=30)\n",
        "\n",
        "chunked_docs = splitter.split_documents(docs)\n",
        "print(f\"Got {len(chunked_docs):d} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixmCdRzBQ5gu"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "db = FAISS.from_documents(\n",
        "    chunked_docs,\n",
        "    HuggingFaceEmbeddings(model_name='BAAI/bge-base-en-v1.5'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mBTreCQ9noHK"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={'k': 4}\n",
        ")\n",
        "\n",
        "print(f\"Question: {question}\")\n",
        "for doc in retriever.get_relevant_documents(question):\n",
        "    print(doc)\n",
        "    print(\"Doc: \", doc.page_content[:100], \"...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L-ggaa763VRo"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_name = 'HuggingFaceH4/zephyr-7b-beta'\n",
        "model_name = 'gpt2'\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cR0k1cRWz8Pm"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"question-answering\",\n",
        "    temperature=0.1,\n",
        "    do_sample=True,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=True,\n",
        "    max_new_tokens=100,\n",
        "    truncation=True,\n",
        ")\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\",\n",
        ")\n",
        "\n",
        "# Function to format input using the chat template\n",
        "def apply_chat_template_with_retriever(question, retriever=retriever):\n",
        "    context_docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([doc.page_content for doc in context_docs])\n",
        "    formatted_prompt = prompt_template.format(context=context, question=question)\n",
        "    return formatted_prompt\n",
        "\n",
        "retriever = db.as_retriever()\n",
        "\n",
        "formatted_prompt = apply_chat_template_with_retriever(question, retriever)\n",
        "\n",
        "# Generate text using the formatted prompt\n",
        "output = text_generation_pipeline(formatted_prompt)\n",
        "print(output[0]['generated_text'])\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "llm_chain = apply_chat_template_with_retriever | llm | StrOutputParser()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "<|system|>\n",
        "Answer the question based on your knowledge. Use the following context to help:\n",
        "\n",
        "{context}\n",
        "\n",
        "</s>\n",
        "<|user|>\n",
        "{question}\n",
        "</s>\n",
        "<|assistant|>\n",
        "\n",
        " \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template,\n",
        ")\n",
        "\n",
        "llm_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result_without_context = llm_chain.invoke({\"context\": \"\", \"question\": question})\n",
        "print(result_without_context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "FZpNA3o10H10",
        "outputId": "31f9aed3-3dd7-4ff8-d1a8-866794fefe80"
      },
      "outputs": [],
      "source": [
        "rag_chain = ({\n",
        "    \"context\": retriever,\n",
        "    \"question\": RunnablePassthrough()\n",
        "    }\n",
        "    | llm_chain\n",
        ")\n",
        "\n",
        "result_with_rag = rag_chain.invoke(question)\n",
        "print(result_with_rag)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
